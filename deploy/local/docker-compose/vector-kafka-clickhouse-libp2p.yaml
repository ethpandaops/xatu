api:
  enabled: true
  address: 0.0.0.0:8686
  playground: true
acknowledgements:
  enabled: true
sources:
  internal_metrics:
    type: internal_metrics
  libp2p_trace_kafka:
    type: kafka
    bootstrap_servers: "${KAFKA_BROKERS}"
    group_id: xatu-vector-kafka-clickhouse-libp2p-trace
    key_field: "event.id"
    decoding:
      codec: json
    topics:
      - "^libp2p-trace.+"
    auto_offset_reset: earliest
    librdkafka_options:
      message.max.bytes: "10485760" # 10MB
transforms:
  xatu_server_events_meta:
    type: remap
    inputs:
      - libp2p_trace_kafka
    source: |-
      .meta_client_name = .meta.client.name
      .meta_client_id = .meta.client.id
      .meta_client_version = .meta.client.version
      .meta_client_implementation = .meta.client.implementation
      .meta_client_os = .meta.client.os
      if exists(.meta.server.client.ip) && is_string(.meta.server.client.ip) {
        if is_ipv4!(.meta.server.client.ip) {
          .meta_client_ip = ip_to_ipv6!(.meta.server.client.ip)
        } else if is_ipv6!(.meta.server.client.ip) {
          .meta_client_ip = .meta.server.client.ip
        }
      }
      if exists(.meta.server.client.geo) {
        .meta_client_geo_city = .meta.server.client.geo.city
        .meta_client_geo_country = .meta.server.client.geo.country
        .meta_client_geo_country_code = .meta.server.client.geo.country_code
        .meta_client_geo_continent_code = .meta.server.client.geo.continent_code
        .meta_client_geo_longitude = .meta.server.client.geo.longitude
        .meta_client_geo_latitude = .meta.server.client.geo.latitude
        .meta_client_geo_autonomous_system_number = .meta.server.client.geo.autonomous_system_number
        .meta_client_geo_autonomous_system_organization = .meta.server.client.geo.autonomous_system_organization
      }
      .meta_network_id = .meta.client.ethereum.network.id
      .meta_network_name = .meta.client.ethereum.network.name
      if exists(.meta.client.ethereum.consensus) {
        .meta_consensus_implementation = .meta.client.ethereum.consensus.implementation
        if is_string(.meta.client.ethereum.consensus.version) {
          version, err = split(.meta.client.ethereum.consensus.version, "/", limit: 3)
          if err == null && length(version) > 1 {
            .meta_consensus_version = version[1]
          }
          if is_string(.meta_consensus_version) {
            sematic_version, err = split(.meta_consensus_version, ".", limit: 3)
            if err == null {
              if sematic_version[0] != null {
                version_major, err = replace(sematic_version[0], "v", "", count: 1)
                if err == null {
                  .meta_consensus_version_major = version_major
                  .meta_consensus_version_minor = sematic_version[1]
                  if sematic_version[2] != null {
                    version_patch, err = replace(sematic_version[2], r'[-+ ](.*)', "")
                    if err == null {
                      .meta_consensus_version_patch = version_patch
                    }
                  }
                }
              }
            }
          }
        }
      }
      if exists(.meta.client.ethereum.execution) {
        if exists(.meta.client.ethereum.execution.fork_id) {
          .meta_execution_fork_id_hash = .meta.client.ethereum.execution.fork_id.hash
          .meta_execution_fork_id_next = .meta.client.ethereum.execution.fork_id.next
        }
      }
      if exists(.meta.client.labels) {
        .meta_labels = .meta.client.labels
      }

      # handle event name pathing and map it back to .data, .meta.client.additional_data, .meta.server.additional_data
      if !exists(.data) {
        data, err = get(value: ., path: [.event.name])
        if err == null {
          .data = data
        } else {
          .error = err
          .error_description = "failed to get data"
          log(., level: "error", rate_limit_secs: 60)
        }

        cleanedUpData, err = remove(value: ., path: [.event.name])
        if err == null {
          . = cleanedUpData
        } else {
          .error = err
          .error_description = "failed to remove data"
          log(., level: "error", rate_limit_secs: 60)
        }

        if exists(.meta.client) {
          clientAdditionalData, err = get(value: .meta.client, path: [.event.name])
          if err == null {
            .meta.client.additional_data = clientAdditionalData
          } else {
            .error = err
            .error_description = "failed to get client additional data"
            log(., level: "error", rate_limit_secs: 60)
          }

          cleanedUpClient, err = remove(value: .meta.client, path: [.event.name])
          if err == null {
            .meta.client = cleanedUpClient
          } else {
            .error = err
            .error_description = "failed to remove client additional data"
            log(., level: "error", rate_limit_secs: 60)
          }
        }

        if exists(.meta.server) {
          serverAdditionalData, err = get(value: .meta.server, path: [.event.name])
          if err == null {
            .meta.server.additional_data = serverAdditionalData
          } else {
            .error = err
            .error_description = "failed to get server additional data"
            log(., level: "error", rate_limit_secs: 60)
          }

          cleanedUpClient, err = remove(value: .meta.server, path: [.event.name])
          if err == null {
            .meta.server = cleanedUpClient
          } else {
            .error = err
            .error_description = "failed to remove server additional data"
            log(., level: "error", rate_limit_secs: 60)
          }
        }
      }
      if exists(.meta.client.additional_data.peer.user_agent) {
        agent, err = split(.meta.client.additional_data.peer.user_agent, "/", limit: 4)
        if err == null && length(agent) > 1 {
          implementation, err = downcase(agent[0])
          if err == null {
            .peer_implementation = implementation
            # handle teku case with teku/teku/v1.1.1
            if agent[0] == agent[1] {
              .peer_version = agent[2]
            } else {
              .peer_version = agent[1]
            }
          }
        }
        if is_string(.peer_version) {
          sematic_version, err = split(.peer_version, ".", limit: 3)
          if err == null {
            if sematic_version[0] != null {
              version_major, err = replace(sematic_version[0], "v", "", count: 1)
              if err == null {
                .peer_version_major = version_major
                .peer_version_minor = sematic_version[1]
                if sematic_version[2] != null {
                  version_patch, err = replace(sematic_version[2], r'[-+ ](.*)', "")
                  if err == null {
                    .peer_version_patch = version_patch
                  }
                }
              }
            }
          }
        }
      }
      if exists(.meta.server.additional_data.peer.geo) {
        .peer_geo_city = .meta.server.additional_data.peer.geo.city
        .peer_geo_country = .meta.server.additional_data.peer.geo.country
        .peer_geo_country_code = .meta.server.additional_data.peer.geo.country_code
        .peer_geo_continent_code = .meta.server.additional_data.peer.geo.continent_code
        .peer_geo_longitude = .meta.server.additional_data.peer.geo.longitude
        .peer_geo_latitude = .meta.server.additional_data.peer.geo.latitude
        .peer_geo_autonomous_system_number = .meta.server.additional_data.peer.geo.autonomous_system_number
        .peer_geo_autonomous_system_organization = .meta.server.additional_data.peer.geo.autonomous_system_organization
      }

      # delete kafka fields
      del(.timestamp)
      del(.topic)
      del(.source_type)
      del(.partition)
      del(.offset)
      del(.message_key)
      del(.headers)
      del(.path)
  xatu_server_events_router:
    type: route
    inputs:
      - xatu_server_events_meta
    route:
      libp2p_trace_connected: .event.name == "LIBP2P_TRACE_CONNECTED"
      libp2p_trace_disconnected: .event.name == "LIBP2P_TRACE_DISCONNECTED"
      libp2p_trace_add_peer: .event.name == "LIBP2P_TRACE_ADD_PEER"
      libp2p_trace_remove_peer: .event.name == "LIBP2P_TRACE_REMOVE_PEER"
      libp2p_trace_recv_rpc: .event.name == "LIBP2P_TRACE_RECV_RPC"
      libp2p_trace_send_rpc: .event.name == "LIBP2P_TRACE_SEND_RPC"
      libp2p_trace_drop_rpc: .event.name == "LIBP2P_TRACE_DROP_RPC"
      libp2p_trace_join: .event.name == "LIBP2P_TRACE_JOIN"
      libp2p_trace_leave: .event.name == "LIBP2P_TRACE_LEAVE"
      libp2p_trace_graft: .event.name == "LIBP2P_TRACE_GRAFT"
      libp2p_trace_prune: .event.name == "LIBP2P_TRACE_PRUNE"
      libp2p_trace_publish_message: .event.name == "LIBP2P_TRACE_PUBLISH_MESSAGE"
      libp2p_trace_reject_message: .event.name == "LIBP2P_TRACE_REJECT_MESSAGE"
      libp2p_trace_duplicate_message: .event.name == "LIBP2P_TRACE_DUPLICATE_MESSAGE"
      libp2p_trace_deliver_message: .event.name == "LIBP2P_TRACE_DELIVER_MESSAGE"
      libp2p_trace_handle_metadata: .event.name == "LIBP2P_TRACE_HANDLE_METADATA"
      libp2p_trace_handle_status: .event.name == "LIBP2P_TRACE_HANDLE_STATUS"
      libp2p_trace_gossipsub_beacon_block: .event.name == "LIBP2P_TRACE_GOSSIPSUB_BEACON_BLOCK"
      libp2p_trace_gossipsub_beacon_attestation: .event.name == "LIBP2P_TRACE_GOSSIPSUB_BEACON_ATTESTATION"
      libp2p_trace_gossipsub_aggregate_and_proof: .event.name == "LIBP2P_TRACE_GOSSIPSUB_AGGREGATE_AND_PROOF"
      libp2p_trace_gossipsub_blob_sidecar: .event.name == "LIBP2P_TRACE_GOSSIPSUB_BLOB_SIDECAR"
      libp2p_trace_gossipsub_data_column_sidecar: .event.name == "LIBP2P_TRACE_GOSSIPSUB_DATA_COLUMN_SIDECAR"
      libp2p_trace_heartbeat: .event.name == "LIBP2P_TRACE_HEARTBEAT"
      libp2p_trace_rpc_meta_control_ihave: .event.name == "LIBP2P_TRACE_RPC_META_CONTROL_IHAVE"
      libp2p_trace_rpc_meta_control_iwant: .event.name == "LIBP2P_TRACE_RPC_META_CONTROL_IWANT"
      libp2p_trace_rpc_meta_control_idontwant: .event.name == "LIBP2P_TRACE_RPC_META_CONTROL_IDONTWANT"
      libp2p_trace_rpc_meta_control_graft: .event.name == "LIBP2P_TRACE_RPC_META_CONTROL_GRAFT"
      libp2p_trace_rpc_meta_control_prune: .event.name == "LIBP2P_TRACE_RPC_META_CONTROL_PRUNE"
      libp2p_trace_rpc_meta_subscription: .event.name == "LIBP2P_TRACE_RPC_META_SUBSCRIPTION"
      libp2p_trace_rpc_meta_message: .event.name == "LIBP2P_TRACE_RPC_META_MESSAGE"
  xatu_server_events_router_matched:
    type: log_to_metric
    inputs:
      - xatu_server_events_router.libp2p_trace_connected
      - xatu_server_events_router.libp2p_trace_disconnected
      - xatu_server_events_router.libp2p_trace_add_peer
      - xatu_server_events_router.libp2p_trace_remove_peer
      - xatu_server_events_router.libp2p_trace_recv_rpc
      - xatu_server_events_router.libp2p_trace_send_rpc
      - xatu_server_events_router.libp2p_trace_drop_rpc
      - xatu_server_events_router.libp2p_trace_join
      - xatu_server_events_router.libp2p_trace_leave
      - xatu_server_events_router.libp2p_trace_graft
      - xatu_server_events_router.libp2p_trace_prune
      - xatu_server_events_router.libp2p_trace_publish_message
      - xatu_server_events_router.libp2p_trace_reject_message
      - xatu_server_events_router.libp2p_trace_duplicate_message
      - xatu_server_events_router.libp2p_trace_deliver_message
      - xatu_server_events_router.libp2p_trace_handle_metadata
      - xatu_server_events_router.libp2p_trace_handle_status
      - xatu_server_events_router.libp2p_trace_gossipsub_beacon_block
      - xatu_server_events_router.libp2p_trace_gossipsub_beacon_attestation
      - xatu_server_events_router.libp2p_trace_gossipsub_aggregate_and_proof
      - xatu_server_events_router.libp2p_trace_gossipsub_blob_sidecar
      - xatu_server_events_router.libp2p_trace_gossipsub_data_column_sidecar
      - xatu_server_events_router.libp2p_trace_heartbeat
      - xatu_server_events_router.libp2p_trace_rpc_meta_control_ihave
      - xatu_server_events_router.libp2p_trace_rpc_meta_control_iwant
      - xatu_server_events_router.libp2p_trace_rpc_meta_control_idontwant
      - xatu_server_events_router.libp2p_trace_rpc_meta_control_graft
      - xatu_server_events_router.libp2p_trace_rpc_meta_control_prune
      - xatu_server_events_router.libp2p_trace_rpc_meta_subscription
      - xatu_server_events_router.libp2p_trace_rpc_meta_message
    metrics:
      - type: counter
        field: event.name
        namespace: xatu
        name: xatu_server_events_matched
        tags:
          event: "{{event.name}}"
          source: "xatu-kafka-clickhouse"
  xatu_server_events_router_unmatched:
    type: log_to_metric
    inputs:
      - xatu_server_events_router._unmatched
    metrics:
      - type: counter
        field: event.name
        namespace: xatu
        name: xatu_server_events_unmatched
        tags:
          event: "{{event.name}}"
          source: "xatu-kafka-clickhouse"
  libp2p_trace_peer_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_connected
      - xatu_server_events_router.libp2p_trace_disconnected
    source: |-
      .peer_id = .data.remote_peer
      key, err = .data.remote_peer + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .unique_key = seahash(key)
      .updated_date_time = to_unix_timestamp(now())
      del(.event)
      del(.meta)
      del(.data)
  libp2p_trace_connected_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_connected
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      peer_id_key, err = .data.remote_peer + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .remote_peer_id_unique_key = seahash(peer_id_key)
      addrParts, err = split(.data.remote_maddrs, "/")
      if err != null {
        .error = err
        .error_description = "failed to split remote_maddrs"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(addrParts) >= 5 {
            .remote_protocol = addrParts[1]
            .remote_ip = addrParts[2]
            .remote_transport_protocol = addrParts[3]
            .remote_port = addrParts[4]
        } else {
            .error_description = "failed to split remote_maddrs"
            log(., level: "error", rate_limit_secs: 60)
        }
      }
      if exists(.meta.server.additional_data.peer.geo) {
        .remote_geo_city = .meta.server.additional_data.peer.geo.city
        .remote_geo_country = .meta.server.additional_data.peer.geo.country
        .remote_geo_country_code = .meta.server.additional_data.peer.geo.country_code
        .remote_geo_continent_code = .meta.server.additional_data.peer.geo.continent_code
        .remote_geo_longitude = .meta.server.additional_data.peer.geo.longitude
        .remote_geo_latitude = .meta.server.additional_data.peer.geo.latitude
        .remote_geo_autonomous_system_number = .meta.server.additional_data.peer.geo.autonomous_system_number
        .remote_geo_autonomous_system_organization = .meta.server.additional_data.peer.geo.autonomous_system_organization
      }
      if is_string(.data.agent_version) {
        agent_version_cleaned = replace(to_string!(.data.agent_version), "teku/teku", "teku", count: 1)
        agent_version = split(agent_version_cleaned, "/", limit: 4)
        if length(agent_version) > 0 {
          if is_string(agent_version[0]) {
            implementation, err = downcase(agent_version[0])
            if err == null {
              .remote_agent_implementation = implementation
            }
          }
        }
        if length(agent_version) > 1 {
          if is_string(agent_version[1]) {
            .remote_agent_version = agent_version[1]
            if is_string(.remote_agent_version) {
              sematic_version, err = split(.remote_agent_version, ".", limit: 3)
              if err == null {
                if sematic_version[0] != null {
                  version_major, err = replace(sematic_version[0], "v", "", count: 1)
                  if err == null {
                    .remote_agent_version_major = version_major
                    .remote_agent_version_minor = sematic_version[1]
                    if sematic_version[2] != null {
                      version_patch, err = replace(sematic_version[2], r'[-+ ](.*)', "")
                      if err == null {
                        .remote_agent_version_patch = version_patch
                      }
                    }
                  }
                }
              }
            }
          }
        }
        if length(agent_version) > 2 {
          if is_string(agent_version[2]) && .remote_agent_implementation != "prysm" {
            .remote_agent_platform = agent_version[2]
          }
        }
      }
      .direction = .data.direction
      opened, err = parse_timestamp(.data.opened, format: "%+")
      if err == null {
        .opened = to_unix_timestamp(opened)
      } else {
        .error = err
        .error_description = "failed to parse opened"
        log(., level: "error", rate_limit_secs: 60)
      }
      .transient = .data.transient
      .updated_date_time = to_unix_timestamp(now())
      del(.event)
      del(.meta)
      del(.data)
  libp2p_trace_disconnected_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_disconnected
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      peer_id_key, err = .data.remote_peer + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .remote_peer_id_unique_key = seahash(peer_id_key)
      addrParts, err = split(.data.remote_maddrs, "/")
      if err != null {
        .error = err
        .error_description = "failed to split remote_maddrs"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(addrParts) >= 5 {
            .remote_protocol = addrParts[1]
            .remote_ip = addrParts[2]
            .remote_transport_protocol = addrParts[3]
            .remote_port = addrParts[4]
        } else {
            .error_description = "failed to split remote_maddrs"
            log(., level: "error", rate_limit_secs: 60)
        }
      }
      if exists(.meta.server.additional_data.peer.geo) {
        .remote_geo_city = .meta.server.additional_data.peer.geo.city
        .remote_geo_country = .meta.server.additional_data.peer.geo.country
        .remote_geo_country_code = .meta.server.additional_data.peer.geo.country_code
        .remote_geo_continent_code = .meta.server.additional_data.peer.geo.continent_code
        .remote_geo_longitude = .meta.server.additional_data.peer.geo.longitude
        .remote_geo_latitude = .meta.server.additional_data.peer.geo.latitude
        .remote_geo_autonomous_system_number = .meta.server.additional_data.peer.geo.autonomous_system_number
        .remote_geo_autonomous_system_organization = .meta.server.additional_data.peer.geo.autonomous_system_organization
      }
      if is_string(.data.agent_version) {
        agent_version_cleaned = replace(to_string!(.data.agent_version), "teku/teku", "teku", count: 1)
        agent_version = split(agent_version_cleaned, "/", limit: 4)
        if length(agent_version) > 0 {
          if is_string(agent_version[0]) {
            implementation, err = downcase(agent_version[0])
            if err == null {
              .remote_agent_implementation = implementation
            }
          }
        }
        if length(agent_version) > 1 {
          if is_string(agent_version[1]) {
            .remote_agent_version = agent_version[1]
            if is_string(.remote_agent_version) {
              sematic_version, err = split(.remote_agent_version, ".", limit: 3)
              if err == null {
                if sematic_version[0] != null {
                  version_major, err = replace(sematic_version[0], "v", "", count: 1)
                  if err == null {
                    .remote_agent_version_major = version_major
                    .remote_agent_version_minor = sematic_version[1]
                    if sematic_version[2] != null {
                      version_patch, err = replace(sematic_version[2], r'[-+ ](.*)', "")
                      if err == null {
                        .remote_agent_version_patch = version_patch
                      }
                    }
                  }
                }
              }
            }
          }
        }
        if length(agent_version) > 2 {
          if is_string(agent_version[2]) && .remote_agent_implementation != "prysm" {
            .remote_agent_platform = agent_version[2]
          }
        }
      }
      .direction = .data.direction
      opened, err = parse_timestamp(.data.opened, format: "%+")
      if err == null {
        .opened = to_unix_timestamp(opened)
      } else {
        .error = err
        .error_description = "failed to parse opened"
        log(., level: "error", rate_limit_secs: 60)
      }
      .transient = .data.transient
      .updated_date_time = to_unix_timestamp(now())
      del(.event)
      del(.meta)
      del(.data)
  libp2p_trace_add_peer_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_add_peer
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)
      .protocol = .data.protocol
      .updated_date_time = to_unix_timestamp(now())
      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_remove_peer_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_remove_peer
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)
      .updated_date_time = to_unix_timestamp(now())
      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_join_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_join
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      topicParts, err = split(.data.topic, "/")
      if err != null {
        .error = err
        .error_description = "failed to split topic"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(topicParts) == 5 {
            .topic_layer = topicParts[1]
            .topic_fork_digest_value = topicParts[2]
            .topic_name = topicParts[3]
            .topic_encoding = topicParts[4]
        } else {
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }
      peer_id_key, err = .meta.client.additional_data.metadata.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)
      .updated_date_time = to_unix_timestamp(now())
      del(.event)
      del(.meta)
      del(.data)
      del(.path)

  libp2p_trace_leave_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_leave
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      topicParts, err = split(.data.topic, "/")
      if err != null {
        .error = err
        .error_description = "failed to split topic"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(topicParts) == 5 {
            .topic_layer = topicParts[1]
            .topic_fork_digest_value = topicParts[2]
            .topic_name = topicParts[3]
            .topic_encoding = topicParts[4]
        } else {
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }
      peer_id_key, err = .meta.client.additional_data.metadata.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)
      .updated_date_time = to_unix_timestamp(now())
      del(.event)
      del(.meta)
      del(.data)
      del(.path)

  libp2p_trace_graft_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_graft
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      topicParts, err = split(.data.topic, "/")
      if err != null {
        .error = err
        .error_description = "failed to split topic"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(topicParts) == 5 {
            .topic_layer = topicParts[1]
            .topic_fork_digest_value = topicParts[2]
            .topic_name = topicParts[3]
            .topic_encoding = topicParts[4]
        } else {
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }
      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)
      .updated_date_time = to_unix_timestamp(now())
      del(.event)
      del(.meta)
      del(.data)
      del(.path)

  libp2p_trace_prune_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_prune
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      topicParts, err = split(.data.topic, "/")
      if err != null {
        .error = err
        .error_description = "failed to split topic"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(topicParts) == 5 {
            .topic_layer = topicParts[1]
            .topic_fork_digest_value = topicParts[2]
            .topic_name = topicParts[3]
            .topic_encoding = topicParts[4]
        } else {
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }
      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)
      .updated_date_time = to_unix_timestamp(now())
      del(.event)
      del(.meta)
      del(.data)
      del(.path)

  libp2p_trace_publish_message_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_publish_message
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      topicParts, err = split(.data.topic, "/")
      if err != null {
        .error = err
        .error_description = "failed to split topic"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(topicParts) == 5 {
            .topic_layer = topicParts[1]
            .topic_fork_digest_value = topicParts[2]
            .topic_name = topicParts[3]
            .topic_encoding = topicParts[4]
        } else {
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }
      .message_id = .data.msg_id
      .updated_date_time = to_unix_timestamp(now())
      del(.event)
      del(.meta)
      del(.data)
      del(.path)

  libp2p_trace_reject_message_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_reject_message
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      topicParts, err = split(.data.topic, "/")
      if err != null {
        .error = err
        .error_description = "failed to split topic"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(topicParts) == 5 {
            .topic_layer = topicParts[1]
            .topic_fork_digest_value = topicParts[2]
            .topic_name = topicParts[3]
            .topic_encoding = topicParts[4]
        } else {
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }
      .seq_number = .data.seq_number
      .local_delivery = .data.local
      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)
      .message_id = .data.msg_id
      .message_size = .data.msg_size
      .reason = .data.reason
      .updated_date_time = to_unix_timestamp(now())
      del(.event)
      del(.meta)
      del(.data)
      del(.path)

  libp2p_trace_duplicate_message_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_duplicate_message
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      topicParts, err = split(.data.topic, "/")
      if err != null {
        .error = err
        .error_description = "failed to split topic"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(topicParts) == 5 {
            .topic_layer = topicParts[1]
            .topic_fork_digest_value = topicParts[2]
            .topic_name = topicParts[3]
            .topic_encoding = topicParts[4]
        } else {
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }
      .seq_number = .data.seq_number
      .local_delivery = .data.local
      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)
      .message_id = .data.msg_id
      .message_size = .data.msg_size
      .updated_date_time = to_unix_timestamp(now())
      del(.event)
      del(.meta)
      del(.data)
      del(.path)

  libp2p_trace_deliver_message_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_deliver_message
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      topicParts, err = split(.data.topic, "/")
      if err != null {
        .error = err
        .error_description = "failed to split topic"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(topicParts) == 5 {
            .topic_layer = topicParts[1]
            .topic_fork_digest_value = topicParts[2]
            .topic_name = topicParts[3]
            .topic_encoding = topicParts[4]
        } else {
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }
      .seq_number = .data.seq_number
      .local_delivery = .data.local
      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)
      .message_id = .data.msg_id
      .message_size = .data.msg_size
      .updated_date_time = to_unix_timestamp(now())
      del(.event)
      del(.meta)
      del(.data)
      del(.path)

  libp2p_trace_gossipsub_beacon_block_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_gossipsub_beacon_block
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      peer_id_key, err = .meta.client.additional_data.metadata.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)

      .proposer_index = .data.proposer_index


      .propagation_slot_start_diff = .meta.client.additional_data.propagation.slot_start_diff
      .block = .data.block

      .slot = .data.slot
      slot_start_date_time, err = parse_timestamp(.meta.client.additional_data.slot.start_date_time, format: "%+");
      if err == null {
        .slot_start_date_time = to_unix_timestamp(slot_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse slot start date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      .epoch = .meta.client.additional_data.epoch.number
      epoch_start_date_time, err = parse_timestamp(.meta.client.additional_data.epoch.start_date_time, format: "%+");
      if err == null {
        .epoch_start_date_time = to_unix_timestamp(epoch_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse epoch start date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      .wallclock_slot = .meta.client.additional_data.wallclock_slot.number
      wallclock_slot_start_date_time, err = parse_timestamp(.meta.client.additional_data.wallclock_slot.start_date_time, format: "%+");
      if err == null {
        .wallclock_slot_start_date_time = to_unix_timestamp(wallclock_slot_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse wallclock slot start date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      .wallclock_epoch = .meta.client.additional_data.epoch.number
      wallclock_epoch_start_date_time, err = parse_timestamp(.meta.client.additional_data.wallclock_epoch.start_date_time, format: "%+");
      if err == null {
        .wallclock_epoch_start_date_time = to_unix_timestamp(wallclock_epoch_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse wallclock epoch start date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      topicParts, err = split(.meta.client.additional_data.topic, "/")
      if err != null {
          .error = err
          .error_description = "failed to split topic"
      } else {
        if length(topicParts) != 5 {
            errDebug = {
                "topic": .meta.client.additional_data.topic,
            }
            .error_description = "failed to split topic"
        }
      }

      .topic_layer = topicParts[1]
      .topic_fork_digest_value = topicParts[2]
      .topic_name = topicParts[3]
      .topic_encoding = topicParts[4]

      .message_size = .meta.client.additional_data.message_size
      .message_id = .meta.client.additional_data.message_id
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_gossipsub_beacon_attestation_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_gossipsub_beacon_attestation
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      peer_id_key, err = .meta.client.additional_data.metadata.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)

      .attesting_validator_index = .meta.client.additional_data.attesting_validator.index
      .attesting_validator_committee_index = .meta.client.additional_data.attesting_validator.committee_index

      .propagation_slot_start_diff = .meta.client.additional_data.propagation.slot_start_diff
      .beacon_block_root = .data.data.beacon_block_root
      .committee_index = .data.data.index

      .slot = .data.data.slot
      slot_start_date_time, err = parse_timestamp(.meta.client.additional_data.slot.start_date_time, format: "%+");
      if err == null {
        .slot_start_date_time = to_unix_timestamp(slot_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse slot start date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      .epoch = .meta.client.additional_data.epoch.number
      epoch_start_date_time, err = parse_timestamp(.meta.client.additional_data.epoch.start_date_time, format: "%+");
      if err == null {
        .epoch_start_date_time = to_unix_timestamp(epoch_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse epoch start date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      .wallclock_slot = .meta.client.additional_data.wallclock_slot.number
      wallclock_slot_start_date_time, err = parse_timestamp(.meta.client.additional_data.wallclock_slot.start_date_time, format: "%+");
      if err == null {
        .wallclock_slot_start_date_time = to_unix_timestamp(wallclock_slot_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse wallclock slot start date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      .wallclock_epoch = .meta.client.additional_data.epoch.number
      wallclock_epoch_start_date_time, err = parse_timestamp(.meta.client.additional_data.wallclock_epoch.start_date_time, format: "%+");
      if err == null {
        .wallclock_epoch_start_date_time = to_unix_timestamp(wallclock_epoch_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse wallclock epoch start date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      .source_epoch = .data.data.source.epoch
      source_epoch_start_date_time, err = parse_timestamp(.meta.client.additional_data.source.epoch.start_date_time, format: "%+");
      if err == null {
          .source_epoch_start_date_time = to_unix_timestamp(source_epoch_start_date_time)
      } else {
          .error = err
          .error_description = "failed to parse source epoch start date time"
          log(., level: "error", rate_limit_secs: 60)
      }
      .source_root = .data.data.source.root
      .target_epoch = .data.data.target.epoch
      target_epoch_start_date_time, err = parse_timestamp(.meta.client.additional_data.target.epoch.start_date_time, format: "%+");
      if err == null {
          .target_epoch_start_date_time = to_unix_timestamp(target_epoch_start_date_time)
      } else {
          .error = err
          .error_description = "failed to parse target epoch start date time"
          log(., level: "error", rate_limit_secs: 60)
      }

      .target_root = .data.data.target.root
      topicParts, err = split(.meta.client.additional_data.topic, "/")
      if err != null {
          .error = err
          .error_description = "failed to split topic"
      } else {
        if length(topicParts) != 5 {
            .errDebug = {
                "topic": .meta.client.additional_data.topic,
            }
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }

      .topic_layer = topicParts[1]
      .topic_fork_digest_value = topicParts[2]
      .topic_name = topicParts[3]
      .topic_encoding = topicParts[4]

      .message_size = .meta.client.additional_data.message_size
      .message_id = .meta.client.additional_data.message_id
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_gossipsub_aggregate_and_proof_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_gossipsub_aggregate_and_proof
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      peer_id_key, err = .meta.client.additional_data.metadata.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)

      # Aggregator specific fields
      .aggregator_index = .meta.client.additional_data.aggregator_index

      # Embedded attestation fields from the aggregate
      .committee_index = .data.message.aggregate.data.index
      .aggregation_bits = .data.message.aggregate.aggregation_bits
      .beacon_block_root = .data.message.aggregate.data.beacon_block_root
      .source_epoch = .data.message.aggregate.data.source.epoch
      .source_root = .data.message.aggregate.data.source.root
      .target_epoch = .data.message.aggregate.data.target.epoch
      .target_root = .data.message.aggregate.data.target.root
      .signature = .data.message.aggregate.signature

      .propagation_slot_start_diff = .meta.client.additional_data.propagation.slot_start_diff

      .slot = .data.message.aggregate.data.slot
      slot_start_date_time, err = parse_timestamp(.meta.client.additional_data.slot.start_date_time, format: "%+");
      if err == null {
        .slot_start_date_time = to_unix_timestamp(slot_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse slot start date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      .epoch = .meta.client.additional_data.epoch.number
      epoch_start_date_time, err = parse_timestamp(.meta.client.additional_data.epoch.start_date_time, format: "%+");
      if err == null {
        .epoch_start_date_time = to_unix_timestamp(epoch_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse epoch start date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      .wallclock_slot = .meta.client.additional_data.wallclock_slot.number
      wallclock_slot_start_date_time, err = parse_timestamp(.meta.client.additional_data.wallclock_slot.start_date_time, format: "%+");
      if err == null {
        .wallclock_slot_start_date_time = to_unix_timestamp(wallclock_slot_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse wallclock slot start date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      .wallclock_epoch = .meta.client.additional_data.wallclock_epoch.number
      wallclock_epoch_start_date_time, err = parse_timestamp(.meta.client.additional_data.wallclock_epoch.start_date_time, format: "%+");
      if err == null {
        .wallclock_epoch_start_date_time = to_unix_timestamp(wallclock_epoch_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse wallclock epoch start date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      topicParts, err = split(.meta.client.additional_data.topic, "/")
      if err != null {
          .error = err
          .error_description = "failed to split topic"
      } else {
        if length(topicParts) != 5 {
            .errDebug = {
                "topic": .meta.client.additional_data.topic,
            }
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }

      .topic_layer = topicParts[1]
      .topic_fork_digest_value = topicParts[2]
      .topic_name = topicParts[3]
      .topic_encoding = topicParts[4]

      .message_size = .meta.client.additional_data.message_size
      .message_id = .meta.client.additional_data.message_id
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)

  libp2p_trace_gossipsub_blob_sidecar_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_gossipsub_blob_sidecar
    source: |-
      .updated_date_time = to_unix_timestamp(now())
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      .slot = .data.slot
      slot_start_date_time, err = parse_timestamp(.meta.client.additional_data.slot.start_date_time, format: "%+");
      if err == null {
        .slot_start_date_time = to_unix_timestamp(slot_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse slot start date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      .epoch = .meta.client.additional_data.epoch.number
      epoch_start_date_time, err = parse_timestamp(.meta.client.additional_data.epoch.start_date_time, format: "%+");
      if err == null {
        .epoch_start_date_time = to_unix_timestamp(epoch_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse epoch start date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      .wallclock_slot = .meta.client.additional_data.wallclock_slot.number
      wallclock_slot_start_date_time, err = parse_timestamp(.meta.client.additional_data.wallclock_slot.start_date_time, format: "%+");
      if err == null {
        .wallclock_slot_start_date_time = to_unix_timestamp(wallclock_slot_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse wallclock slot start date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      .wallclock_epoch = .meta.client.additional_data.epoch.number
      wallclock_epoch_start_date_time, err = parse_timestamp(.meta.client.additional_data.wallclock_epoch.start_date_time, format: "%+");
      if err == null {
        .wallclock_epoch_start_date_time = to_unix_timestamp(wallclock_epoch_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse wallclock epoch start date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      .propagation_slot_start_diff = .meta.client.additional_data.propagation.slot_start_diff
      .proposer_index = .data.proposer_index
      .blob_index = .data.index
      .state_root = .data.state_root
      .parent_root = .data.parent_root

      peer_id_key, err = .meta.client.additional_data.metadata.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)
      .message_id = .meta.client.additional_data.message_id
      .message_size = .meta.client.additional_data.message_size

      topicParts, err = split(.meta.client.additional_data.topic, "/")
      if err != null {
          .error = err
          .error_description = "failed to split topic"
      } else {
        if length(topicParts) != 5 {
            .errDebug = {
                "topic": .meta.client.additional_data.topic,
            }
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }

      .topic_layer = topicParts[1]
      .topic_fork_digest_value = topicParts[2]
      .topic_name = topicParts[3]
      .topic_encoding = topicParts[4]

      del(.event)
      del(.meta)
      del(.data)
      del(.path)

  libp2p_trace_gossipsub_data_column_sidecar_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_gossipsub_data_column_sidecar
    source: |-
      .updated_date_time = to_unix_timestamp(now())
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      .slot = .data.slot
      slot_start_date_time, err = parse_timestamp(.meta.client.additional_data.slot.start_date_time, format: "%+");
      if err == null {
        .slot_start_date_time = to_unix_timestamp(slot_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse slot start date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      .epoch = .meta.client.additional_data.epoch.number
      epoch_start_date_time, err = parse_timestamp(.meta.client.additional_data.epoch.start_date_time, format: "%+");
      if err == null {
        .epoch_start_date_time = to_unix_timestamp(epoch_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse epoch start date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      .wallclock_slot = .meta.client.additional_data.wallclock_slot.number
      wallclock_slot_start_date_time, err = parse_timestamp(.meta.client.additional_data.wallclock_slot.start_date_time, format: "%+");
      if err == null {
        .wallclock_slot_start_date_time = to_unix_timestamp(wallclock_slot_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse wallclock slot start date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      .wallclock_epoch = .meta.client.additional_data.epoch.number
      wallclock_epoch_start_date_time, err = parse_timestamp(.meta.client.additional_data.wallclock_epoch.start_date_time, format: "%+");
      if err == null {
        .wallclock_epoch_start_date_time = to_unix_timestamp(wallclock_epoch_start_date_time)
      } else {
        .error = err
        .error_description = "failed to parse wallclock epoch start date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      .propagation_slot_start_diff = .meta.client.additional_data.propagation.slot_start_diff
      .proposer_index = .data.proposer_index
      .column_index = .data.index
      .kzg_commitments_count = .data.kzg_commitments_count
      .state_root = .data.state_root
      .parent_root = .data.parent_root

      peer_id_key, err = .meta.client.additional_data.metadata.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)
      .message_id = .meta.client.additional_data.message_id
      .message_size = .meta.client.additional_data.message_size

      topicParts, err = split(.meta.client.additional_data.topic, "/")
      if err != null {
          .error = err
          .error_description = "failed to split topic"
      } else {
        if length(topicParts) != 5 {
            .errDebug = {
                "topic": .meta.client.additional_data.topic,
            }
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }

      .topic_layer = topicParts[1]
      .topic_fork_digest_value = topicParts[2]
      .topic_name = topicParts[3]
      .topic_encoding = topicParts[4]

      del(.event)
      del(.meta)
      del(.data)
      del(.path)

  libp2p_trace_handle_status_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_handle_status
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)

      if .data.error != null {
        .error = .data.error
      }

      if .data.request != null {
        .request_finalized_epoch = .data.request.finalized_epoch
        .request_finalized_root = .data.request.finalized_root
        .request_fork_digest = .data.request.fork_digest
        .request_head_root = .data.request.head_root
        .request_head_slot = .data.request.head_slot
      }
      if .data.response != null {
        .response_finalized_epoch = .data.response.finalized_epoch
        .response_finalized_root = .data.response.finalized_root
        .response_fork_digest = .data.response.fork_digest
        .response_head_root = .data.response.head_root
        .response_head_slot = .data.response.head_slot
      }

      .latency_milliseconds, err = .data.latency * 1000
      if err != null {
        .error = err
        .error_description = "failed to convert latency to millseconds"
        log(., level: "error", rate_limit_secs: 60)
      }

      .protocol = .data.protocol_id

      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_handle_metadata_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_handle_metadata
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)
      .unique_key = seahash(.event.id)

      if .data.error != null {
        .error = .data.error
      }

      .attnets = .data.metadata.attnets
      .seq_number = .data.metadata.seq_number
      .syncnets = .data.metadata.syncnets

      .latency_milliseconds, err = .data.latency * 1000
      if err != null {
        .error = err
        .error_description = "failed to convert latency to millseconds"
        log(., level: "error", rate_limit_secs: 60)
      }

      .protocol = .data.protocol_id
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_rpc_meta_control_ihave_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_rpc_meta_control_ihave
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      rootEventKey = seahash(.data.root_event_id)
      if err != null {
        .error = err
        .error_description = "failed to generate root event key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .rpc_meta_unique_key = rootEventKey

      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)

      topicParts, err = split(.data.topic, "/")
      if err != null {
        .error = err
        .error_description = "failed to split topic"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(topicParts) == 5 {
            .topic_layer = topicParts[1]
            .topic_fork_digest_value = topicParts[2]
            .topic_name = topicParts[3]
            .topic_encoding = topicParts[4]
        } else {
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }

      key, err = to_string(rootEventKey) + "_rpc_meta_control_ihave_" + to_string(.data.control_index) + "_" + to_string(.data.message_index)
      if err != null {
        .errDebug = {
          "root_event_key": rootEventKey,
          "message_index": .data.message_index,
          "control_index": .data.control_index,
        }
        .error = "failed to generate unique key"
        .error_description = "failed to generate unique key for rpc_meta_control_ihave"
      }
      .unique_key = seahash(key)

      .control_index = .data.control_index
      .message_index = .data.message_index
      .message_id = .data.message_id
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_rpc_meta_control_iwant_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_rpc_meta_control_iwant
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      rootEventKey = seahash(.data.root_event_id)
      if err != null {
        .error = err
        .error_description = "failed to generate root event key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .rpc_meta_unique_key = rootEventKey

      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)

      key, err = to_string(rootEventKey) + "_rpc_meta_control_iwant_" + to_string(.data.control_index) + "_" + to_string(.data.message_index)
      if err != null {
        .errDebug = {
          "root_event_key": rootEventKey,
          "message_index": .data.message_index,
          "control_index": .data.control_index,
        }
        .error = "failed to generate unique key"
        .error_description = "failed to generate unique key for rpc_meta_control_iwant"
      }
      .unique_key = seahash(key)

      .control_index = .data.control_index
      .message_index = .data.message_index
      .message_id = .data.message_id
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_rpc_meta_control_idontwant_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_rpc_meta_control_idontwant
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      rootEventKey = seahash(.data.root_event_id)
      if err != null {
        .error = err
        .error_description = "failed to generate root event key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .rpc_meta_unique_key = rootEventKey

      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)

      key, err = to_string(rootEventKey) + "_rpc_meta_control_idontwant_" + to_string(.data.control_index) + "_" + to_string(.data.message_index)
      if err != null {
        .errDebug = {
          "root_event_key": rootEventKey,
          "message_index": .data.message_index,
          "control_index": .data.control_index,
        }
        .error = "failed to generate unique key"
        .error_description = "failed to generate unique key for rpc_meta_control_idontwant"
      }
      .unique_key = seahash(key)

      .control_index = .data.control_index
      .message_index = .data.message_index
      .message_id = .data.message_id
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_rpc_meta_control_graft_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_rpc_meta_control_graft
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      rootEventKey = seahash(.data.root_event_id)
      if err != null {
        .error = err
        .error_description = "failed to generate root event key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .rpc_meta_unique_key = rootEventKey

      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)

      topicParts, err = split(.data.topic, "/")
      if err != null {
        .error = err
        .error_description = "failed to split topic"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(topicParts) == 5 {
            .topic_layer = topicParts[1]
            .topic_fork_digest_value = topicParts[2]
            .topic_name = topicParts[3]
            .topic_encoding = topicParts[4]
        } else {
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }

      key, err = to_string(rootEventKey) + "_rpc_meta_control_graft_" + to_string(.data.control_index) + "_" + to_string(.data.topic)
      if err != null {
        .errDebug = {
          "root_event_key": rootEventKey,
          "topic": .data.topic,
          "control_index": .data.control_index,
        }
        .error = "failed to generate unique key"
        .error_description = "failed to generate unique key for rpc_meta_control_graft"
      }
      .unique_key = seahash(key)

      .control_index = .data.control_index
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_rpc_meta_control_prune_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_rpc_meta_control_prune
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      rootEventKey = seahash(.data.root_event_id)
      if err != null {
        .error = err
        .error_description = "failed to generate root event key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .rpc_meta_unique_key = rootEventKey

      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)

      topicParts, err = split(.data.topic, "/")
      if err != null {
        .error = err
        .error_description = "failed to split topic"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(topicParts) == 5 {
            .topic_layer = topicParts[1]
            .topic_fork_digest_value = topicParts[2]
            .topic_name = topicParts[3]
            .topic_encoding = topicParts[4]
        } else {
            .error_description = "failed to split topic"
            log(., level: "error", rate_limit_secs: 60)
        }
      }

      graft_peer_id_key, err = .data.graft_peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate graft peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .graft_peer_id_unique_key = seahash(graft_peer_id_key)

      key, err = to_string(rootEventKey) + "_rpc_meta_control_prune_" + to_string(.data.control_index) + "_" + to_string(.data.peer_index)
      if err != null {
        .errDebug = {
          "root_event_key": rootEventKey,
          "peer_index": .data.peer_index,
          "control_index": .data.control_index,
        }
        .error = "failed to generate unique key"
        .error_description = "failed to generate unique key for rpc_meta_control_prune"
      }
      .unique_key = seahash(key)

      .control_index = .data.control_index
      .peer_id_index = .data.peer_index
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_rpc_meta_subscription_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_rpc_meta_subscription
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      rootEventKey = seahash(.data.root_event_id)
      if err != null {
        .error = err
        .error_description = "failed to generate root event key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .rpc_meta_unique_key = rootEventKey

      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)

      topicParts, err = split(.data.topic_id, "/")
      if err != null {
        .error = err
        .error_description = "failed to split topic_id"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(topicParts) == 5 {
            .topic_layer = topicParts[1]
            .topic_fork_digest_value = topicParts[2]
            .topic_name = topicParts[3]
            .topic_encoding = topicParts[4]
        } else {
            .error_description = "failed to split topic_id"
            log(., level: "error", rate_limit_secs: 60)
        }
      }

      key, err = to_string(rootEventKey) + "_rpc_meta_subscription_" + to_string(.data.control_index) + "_" + to_string(.data.topic_id)
      if err != null {
        .errDebug = {
          "root_event_key": rootEventKey,
          "topic_id": .data.topic_id,
          "control_index": .data.control_index,
        }
        .error = "failed to generate unique key"
        .error_description = "failed to generate unique key for rpc_meta_subscription"
      }
      .unique_key = seahash(key)

      .control_index = .data.control_index
      .subscribe = .data.subscribe
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_rpc_meta_message_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_rpc_meta_message
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      rootEventKey = seahash(.data.root_event_id)
      if err != null {
        .error = err
        .error_description = "failed to generate root event key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .rpc_meta_unique_key = rootEventKey

      peer_id_key, err = .data.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)

      topicParts, err = split(.data.topic_id, "/")
      if err != null {
        .error = err
        .error_description = "failed to split topic_id"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(topicParts) == 5 {
            .topic_layer = topicParts[1]
            .topic_fork_digest_value = topicParts[2]
            .topic_name = topicParts[3]
            .topic_encoding = topicParts[4]
        } else {
            .error_description = "failed to split topic_id"
            log(., level: "error", rate_limit_secs: 60)
        }
      }

      key, err = to_string(rootEventKey) + "_rpc_meta_message_" + to_string(.data.control_index) + "_" + to_string(.data.topic_id)
      if err != null {
        .errDebug = {
          "root_event_key": rootEventKey,
          "topic_id": .data.topic_id,
          "control_index": .data.control_index,
        }
        .error = "failed to generate unique key"
        .error_description = "failed to generate unique key for rpc_meta_message"
      }
      .unique_key = seahash(key)

      .control_index = .data.control_index
      .message_id = .data.message_id
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_recv_rpc_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_recv_rpc
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      peer_id_key, err = .data.meta.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)

      .unique_key = seahash(.event.id)
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_send_rpc_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_send_rpc
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      peer_id_key, err = .data.meta.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)

      .unique_key = seahash(.event.id)
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)
  libp2p_trace_drop_rpc_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_drop_rpc
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }

      peer_id_key, err = .data.meta.peer_id + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .peer_id_unique_key = seahash(peer_id_key)

      .unique_key = seahash(.event.id)
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)

  libp2p_trace_heartbeat_formatted:
    type: remap
    inputs:
      - xatu_server_events_router.libp2p_trace_heartbeat
    source: |-
      event_date_time, err = parse_timestamp(.event.date_time, format: "%+");
      if err == null {
        .event_date_time = to_unix_timestamp(event_date_time, unit: "milliseconds")
      } else {
        .error = err
        .error_description = "failed to parse event date time"
        log(., level: "error", rate_limit_secs: 60)
      }
      peer_id_key, err = .data.remote_peer + .meta_network_name
      if err != null {
        .error = err
        .error_description = "failed to generate peer id unique key"
        log(., level: "error", rate_limit_secs: 60)
      }
      .remote_peer_id_unique_key = seahash(peer_id_key)
      .remote_maddrs = .data.remote_maddrs
      .latency_ms = to_int(.data.latency_ms)
      .direction = if .data.direction == 1 {
        "inbound"
      } else if .data.direction == 2 {
        "outbound"
      } else {
        "unknown"
      }
      .protocols = .data.protocols

      # Convert connection_age_ns from nanoseconds to milliseconds
      connection_age_ns = .data.connection_age_ns
      if connection_age_ns != null {
        # connection_age_ns is a Go time.Duration (int64 nanoseconds)
        # Convert to milliseconds by dividing by 1,000,000
        .connection_age_ms = to_int!(connection_age_ns) / 1000000
      }

      # Extract IP and port from multiaddr if available
      addrParts, err = split(.data.remote_maddrs, "/")
      if err != null {
        .error = err
        .error_description = "failed to split remote_maddrs"
        log(., level: "error", rate_limit_secs: 60)
      } else {
        if length(addrParts) >= 5 {
          .remote_ip = addrParts[2]
          .remote_port = to_int(addrParts[4])
        } else {
          .error_description = "failed to extract IP from remote_maddrs"
          log(., level: "error", rate_limit_secs: 60)
        }
      }

      # Remote peer geo data from server metadata
      if exists(.meta.server.additional_data.peer.geo) {
        .remote_geo_city = .meta.server.additional_data.peer.geo.city
        .remote_geo_country = .meta.server.additional_data.peer.geo.country
        .remote_geo_country_code = .meta.server.additional_data.peer.geo.country_code
        .remote_geo_continent_code = .meta.server.additional_data.peer.geo.continent_code
        .remote_geo_longitude = .meta.server.additional_data.peer.geo.longitude
        .remote_geo_latitude = .meta.server.additional_data.peer.geo.latitude
        .remote_geo_autonomous_system_number = .meta.server.additional_data.peer.geo.autonomous_system_number
        .remote_geo_autonomous_system_organization = .meta.server.additional_data.peer.geo.autonomous_system_organization
      }

      # Agent version parsing
      if is_string(.data.agent_version) {
        agent_version_cleaned = replace(to_string!(.data.agent_version), "teku/teku", "teku", count: 1)
        agent_version = split(agent_version_cleaned, "/", limit: 4)
        if length(agent_version) > 0 {
          if is_string(agent_version[0]) {
            .agent_version = agent_version[0]
          }
        }
      }

      # Ensure all required LowCardinality geo fields have values (not null/undefined)
      # Remote geo fields (these come from server-side geo enrichment)
      .remote_geo_city = .remote_geo_city || ""
      .remote_geo_country = .remote_geo_country || ""
      .remote_geo_country_code = .remote_geo_country_code || ""
      .remote_geo_continent_code = .remote_geo_continent_code || ""

      # Client geo fields (these come from global xatu_server_events_meta transform)
      .meta_client_geo_city = .meta_client_geo_city || ""
      .meta_client_geo_country = .meta_client_geo_country || ""
      .meta_client_geo_country_code = .meta_client_geo_country_code || ""
      .meta_client_geo_continent_code = .meta_client_geo_continent_code || ""

      .unique_key = seahash(.event.id)
      .updated_date_time = to_unix_timestamp(now())

      del(.event)
      del(.meta)
      del(.data)
      del(.path)

sinks:
  metrics:
    type: prometheus_exporter
    address: 0.0.0.0:9598
    inputs:
      - xatu_server_events_router_matched
      - xatu_server_events_router_unmatched
      - internal_metrics
  libp2p_trace_peer_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_peer_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_peer
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_connected_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_connected_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_connected
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_disconnected_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_disconnected_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_disconnected
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_add_peer_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_add_peer_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_add_peer
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_removed_peer_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_remove_peer_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_remove_peer
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_join_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_join_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_join
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_leave_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_leave_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_leave
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_graft_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_graft_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_graft
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_prune_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_prune_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_prune
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_publish_message_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_publish_message_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_publish_message
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_reject_message_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_reject_message_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_reject_message
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_duplicate_message_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_duplicate_message_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_duplicate_message
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_deliver_message_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_deliver_message_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_deliver_message
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_rpc_meta_control_prune_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_rpc_meta_control_prune_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_rpc_meta_control_prune
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_rpc_meta_control_graft_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_rpc_meta_control_graft_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_rpc_meta_control_graft
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_rpc_meta_control_iwant_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_rpc_meta_control_iwant_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_rpc_meta_control_iwant
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_rpc_meta_control_ihave_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_rpc_meta_control_ihave_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_rpc_meta_control_ihave
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_rpc_meta_control_idontwant_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_rpc_meta_control_idontwant_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_rpc_meta_control_idontwant
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_recv_rpc_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_recv_rpc_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_recv_rpc
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_send_rpc_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_send_rpc_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_send_rpc
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_drop_rpc_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_drop_rpc_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_drop_rpc
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_handle_status_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_handle_status_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_handle_status
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_handle_metadata_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_handle_metadata_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_handle_metadata
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_gossipsub_beacon_block_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_gossipsub_beacon_block_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_gossipsub_beacon_block
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_gossipsub_beacon_attestation_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_gossipsub_beacon_attestation_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_gossipsub_beacon_attestation
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_gossipsub_aggregate_and_proof_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_gossipsub_aggregate_and_proof_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_gossipsub_aggregate_and_proof
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_gossipsub_blob_sidecar_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_gossipsub_blob_sidecar_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_gossipsub_blob_sidecar
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_gossipsub_data_column_sidecar_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_gossipsub_data_column_sidecar_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_gossipsub_data_column_sidecar
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_heartbeat_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_heartbeat_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_heartbeat
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_rpc_meta_subscription_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_rpc_meta_subscription_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_rpc_meta_subscription
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
  libp2p_trace_rpc_meta_message_clickhouse:
    type: clickhouse
    inputs:
      - libp2p_trace_rpc_meta_message_formatted
    database: default
    endpoint: "${CLICKHOUSE_ENDPOINT}"
    table: libp2p_rpc_meta_message
    auth:
      strategy: basic
      user: "${CLICKHOUSE_USER}"
      password: "${CLICKHOUSE_PASSWORD}"
    batch:
      max_bytes: 52428800
      max_events: 200000
      timeout_secs: 1
    buffer:
      max_events: 200000
    healthcheck:
      enabled: true
    skip_unknown_fields: false
