# Xatu Sentry Logs - Vector Configuration
# Collects Ethereum execution client structured logs and sends them to Xatu server.
#
# Architecture:
#   1. Sources: File inputs for each client type (ethereum_*)
#   2. Parse: Multi-format parser (raw JSON, slog JSON, terminal, logfmt)
#      then nested JSON handling for double-encoded messages
#   3. Detect: Identify client type from log format
#   4. Normalize: Client-specific transforms to common schema
#   5. Route: Route normalized events by type
#   6. Build: Construct Xatu DecoratedEvent
#   7. Sink: Send to xatu-server HTTP ingester
#
# Supported log formats:
#   - Raw JSON:  {"level":"warn","msg":"Slow block",...}
#   - slog JSON: {"t":"...","lvl":"warn","msg":"{\"level\":\"warn\",...}"}
#   - Terminal:  WARN [01-28|12:58:41.123] {"level":"warn","msg":"Slow block",...}
#   - Logfmt:    t=2026-01-28T... lvl=warn msg="{\"level\":\"warn\",...}"
#
# Adding a new client:
#   1. Add detection condition in `detect_client`
#   2. Add normalizer transform (e.g., `normalize_<client>_block_metrics`)
#   3. Add normalizer to `route_normalized_events` inputs

data_dir: /var/lib/vector

api:
  enabled: true
  address: 0.0.0.0:8686

sources:
  internal_metrics:
    type: internal_metrics

transforms:
  # ============================================================================
  # Stage 1: Multi-Format Log Parser
  # Handles raw JSON, slog JSON, terminal, and logfmt formats.
  # All formats produce the same JSON object for downstream stages.
  # ============================================================================

  parse_json:
    type: remap
    inputs: ["ethereum_*"]
    drop_on_error: true
    source: |
      raw = string!(.message)

      # Strategy 1: Direct JSON parse (raw JSON and --log.format json)
      parsed, err = parse_json(raw)
      if err == null {
        . = parsed
      } else {
        # Strategy 2: Terminal format — "LEVEL [timestamp] {json...}"
        matched, regex_err = parse_regex(raw, r'^[A-Z]+\s+\[[^\]]+\]\s*(?P<json>\{.+\})\s*$')
        if regex_err == null {
          . = parse_json!(string!(matched.json))
        } else {
          # Strategy 3: Logfmt — extract msg field, parse as JSON
          logfmt_parsed, logfmt_err = parse_logfmt(raw)
          if logfmt_err == null {
            msg_val = string(logfmt_parsed.msg) ?? ""
            . = parse_json!(msg_val)
          } else {
            abort
          }
        }
      }

  # Handle double-encoded JSON (some clients output JSON as string in msg field)
  parse_nested_json:
    type: remap
    inputs: ["parse_json"]
    drop_on_error: false
    source: |
      msg_str = string(.msg) ?? ""
      if starts_with(msg_str, "{") {
        parsed = parse_json(msg_str) ?? null
        if parsed != null {
          . = parsed
        }
      }

  # ============================================================================
  # Stage 2: Build Client Metadata
  # ============================================================================

  build_client_meta:
    type: remap
    inputs: ["parse_nested_json"]
    source: |
      .original = .
      .client_name = get_env_var!("XATU_CLIENT_NAME")
      .client_version = get_env_var("XATU_VERSION") ?? "dev"
      .client_implementation = "vector"
      .network_name = get_env_var!("XATU_NETWORK_NAME")
      .network_id_str = get_env_var("XATU_NETWORK_ID") ?? "0"
      .network_id = to_int!(.network_id_str)

  # ============================================================================
  # Stage 3: Detect Client Type and Route to Normalizers
  # ============================================================================

  # Route to client-specific normalizers based on log format
  detect_client:
    type: route
    inputs: ["build_client_meta"]
    route:
      # Geth: "Slow block" message with block/timing/throughput structure
      geth_block_metrics: .original.msg == "Slow block" && exists(.original.block) && exists(.original.timing)
      # Geth: "State metrics" message with delta/depth structure (statesize tracer)
      geth_state_metrics: .original.msg == "State metrics" && exists(.original.delta) && exists(.original.depth)
      # Reth: Add detection pattern when available
      # reth_block_metrics: .original.target == "reth::block" && exists(.original.metrics)
      # Besu: Add detection pattern when available
      # besu_block_metrics: exists(.original.blockNumber) && exists(.original.executionTime)

  # ============================================================================
  # Stage 4: Client-Specific Normalizers
  # Convert client-specific formats to common EXECUTION_BLOCK_METRICS schema
  # ============================================================================

  # Geth normalizer: Converts Geth's "Slow block" log to common schema
  # Note: Core fields (block, timing) are guaranteed by routing condition in detect_client
  # Optional fields (state_reads, state_writes, cache) use get() with defaults
  normalize_geth_block_metrics:
    type: remap
    inputs: ["detect_client.geth_block_metrics"]
    source: |
      log = .original

      # Extract core fields (guaranteed by routing condition)
      block_number = to_string(log.block.number) ?? ""
      block_hash = string(log.block.hash) ?? ""
      gas_used = to_string(log.block.gas_used) ?? ""
      tx_count = int(log.block.tx_count) ?? 0

      # Timing fields (guaranteed by routing condition)
      execution_ms = float(log.timing.execution_ms) ?? 0.0
      state_read_ms = float(log.timing.state_read_ms) ?? 0.0
      state_hash_ms = float(log.timing.state_hash_ms) ?? 0.0
      commit_ms = float(log.timing.commit_ms) ?? 0.0
      total_ms = float(log.timing.total_ms) ?? 0.0

      # Throughput (guaranteed by routing condition check for timing)
      mgas_per_sec = float(log.throughput.mgas_per_sec) ?? 0.0

      # Optional state_reads fields
      sr_accounts = to_string(get(log, ["state_reads", "accounts"]) ?? 0) ?? "0"
      sr_storage = to_string(get(log, ["state_reads", "storage_slots"]) ?? 0) ?? "0"
      sr_code = to_string(get(log, ["state_reads", "code"]) ?? 0) ?? "0"
      sr_code_bytes = to_string(get(log, ["state_reads", "code_bytes"]) ?? 0) ?? "0"

      # Optional state_writes fields
      sw_accounts = to_string(get(log, ["state_writes", "accounts"]) ?? 0) ?? "0"
      sw_accounts_del = to_string(get(log, ["state_writes", "accounts_deleted"]) ?? 0) ?? "0"
      sw_storage = to_string(get(log, ["state_writes", "storage_slots"]) ?? 0) ?? "0"
      sw_storage_del = to_string(get(log, ["state_writes", "storage_slots_deleted"]) ?? 0) ?? "0"
      sw_code = to_string(get(log, ["state_writes", "code"]) ?? 0) ?? "0"
      sw_code_bytes = to_string(get(log, ["state_writes", "code_bytes"]) ?? 0) ?? "0"

      # Optional cache fields
      ac_hits = to_string(get(log, ["cache", "account", "hits"]) ?? 0) ?? "0"
      ac_misses = to_string(get(log, ["cache", "account", "misses"]) ?? 0) ?? "0"
      ac_hit_rate = float(get(log, ["cache", "account", "hit_rate"]) ?? 0) ?? 0.0

      stc_hits = to_string(get(log, ["cache", "storage", "hits"]) ?? 0) ?? "0"
      stc_misses = to_string(get(log, ["cache", "storage", "misses"]) ?? 0) ?? "0"
      stc_hit_rate = float(get(log, ["cache", "storage", "hit_rate"]) ?? 0) ?? 0.0

      cc_hits = to_string(get(log, ["cache", "code", "hits"]) ?? 0) ?? "0"
      cc_misses = to_string(get(log, ["cache", "code", "misses"]) ?? 0) ?? "0"
      cc_hit_rate = float(get(log, ["cache", "code", "hit_rate"]) ?? 0) ?? 0.0
      cc_hit_bytes = to_string(get(log, ["cache", "code", "hit_bytes"]) ?? 0) ?? "0"
      cc_miss_bytes = to_string(get(log, ["cache", "code", "miss_bytes"]) ?? 0) ?? "0"

      # Build normalized schema
      .normalized = {
        "block_number": block_number,
        "block_hash": block_hash,
        "gas_used": gas_used,
        "tx_count": tx_count,
        "execution_ms": execution_ms,
        "state_read_ms": state_read_ms,
        "state_hash_ms": state_hash_ms,
        "commit_ms": commit_ms,
        "total_ms": total_ms,
        "mgas_per_sec": mgas_per_sec,
        "state_reads": {
          "accounts": sr_accounts,
          "storage_slots": sr_storage,
          "code": sr_code,
          "code_bytes": sr_code_bytes
        },
        "state_writes": {
          "accounts": sw_accounts,
          "accounts_deleted": sw_accounts_del,
          "storage_slots": sw_storage,
          "storage_slots_deleted": sw_storage_del,
          "code": sw_code,
          "code_bytes": sw_code_bytes
        },
        "account_cache": {
          "hits": ac_hits,
          "misses": ac_misses,
          "hit_rate": ac_hit_rate
        },
        "storage_cache": {
          "hits": stc_hits,
          "misses": stc_misses,
          "hit_rate": stc_hit_rate
        },
        "code_cache": {
          "hits": cc_hits,
          "misses": cc_misses,
          "hit_rate": cc_hit_rate,
          "hit_bytes": cc_hit_bytes,
          "miss_bytes": cc_miss_bytes
        }
      }
      .event_type = "EXECUTION_BLOCK_METRICS"

  # Geth normalizer: Converts Geth's "State metrics" log to common fields
  # This produces two events (state_size_delta + mpt_depth) via separate builders
  normalize_geth_state_metrics:
    type: remap
    inputs: ["detect_client.geth_state_metrics"]
    source: |
      log = .original

      # Common fields
      .block_number = to_string(log.block_number) ?? ""
      .state_root = string(log.state_root) ?? ""
      .parent_state_root = string(log.parent_state_root) ?? ""

      # Delta fields
      .delta = {
        "account": to_string(get(log, ["delta", "account"]) ?? 0) ?? "0",
        "account_bytes": to_string(get(log, ["delta", "account_bytes"]) ?? 0) ?? "0",
        "account_trienode": to_string(get(log, ["delta", "account_trienode"]) ?? 0) ?? "0",
        "account_trienode_bytes": to_string(get(log, ["delta", "account_trienode_bytes"]) ?? 0) ?? "0",
        "contract_code": to_string(get(log, ["delta", "contract_code"]) ?? 0) ?? "0",
        "contract_code_bytes": to_string(get(log, ["delta", "contract_code_bytes"]) ?? 0) ?? "0",
        "storage": to_string(get(log, ["delta", "storage"]) ?? 0) ?? "0",
        "storage_bytes": to_string(get(log, ["delta", "storage_bytes"]) ?? 0) ?? "0",
        "storage_trienode": to_string(get(log, ["delta", "storage_trienode"]) ?? 0) ?? "0",
        "storage_trienode_bytes": to_string(get(log, ["delta", "storage_trienode_bytes"]) ?? 0) ?? "0"
      }

      # Depth fields - pass through as-is for map columns
      .depth = log.depth

  # ============================================================================
  # Stage 5: Route Normalized Events by Type
  # ============================================================================

  route_normalized_events:
    type: route
    inputs:
      - normalize_geth_block_metrics
      # Add other normalizers here as they're implemented:
      # - normalize_reth_block_metrics
      # - normalize_besu_block_metrics
    route:
      block_metrics: .event_type == "EXECUTION_BLOCK_METRICS"
      # Add other event types here:
      # state_metrics: .event_type == "EXECUTION_STATE_METRICS"

  # ============================================================================
  # Stage 6: Build Xatu Events from Normalized Data
  # ============================================================================

  build_block_metrics_event:
    type: remap
    inputs: ["route_normalized_events.block_metrics"]
    source: |
      .event_data = .normalized
      .event_data.source = "client-logs"
      .event_name = "EXECUTION_BLOCK_METRICS"
      .event_time = format_timestamp!(now(), format: "%Y-%m-%dT%H:%M:%S%.fZ")

  # State size delta event builder: extracts delta fields from state metrics
  build_state_size_delta_event:
    type: remap
    inputs: ["normalize_geth_state_metrics"]
    source: |
      .event_data = {
        "source": "client-logs",
        "block_number": .block_number,
        "state_root": .state_root,
        "parent_state_root": .parent_state_root,
        "account_delta": .delta.account,
        "account_bytes_delta": .delta.account_bytes,
        "account_trienode_delta": .delta.account_trienode,
        "account_trienode_bytes_delta": .delta.account_trienode_bytes,
        "contract_code_delta": .delta.contract_code,
        "contract_code_bytes_delta": .delta.contract_code_bytes,
        "storage_delta": .delta.storage,
        "storage_bytes_delta": .delta.storage_bytes,
        "storage_trienode_delta": .delta.storage_trienode,
        "storage_trienode_bytes_delta": .delta.storage_trienode_bytes
      }
      .event_name = "EXECUTION_STATE_SIZE_DELTA"
      .event_time = format_timestamp!(now(), format: "%Y-%m-%dT%H:%M:%S%.fZ")

  # MPT depth event builder: extracts depth fields from state metrics
  build_mpt_depth_event:
    type: remap
    inputs: ["normalize_geth_state_metrics"]
    source: |
      depth = .depth
      .event_data = {
        "source": "client-logs",
        "block_number": .block_number,
        "state_root": .state_root,
        "parent_state_root": .parent_state_root,
        "total_account_written_nodes": to_string(get(depth, ["total_account_written_nodes"]) ?? 0) ?? "0",
        "total_account_written_bytes": to_string(get(depth, ["total_account_written_bytes"]) ?? 0) ?? "0",
        "total_account_deleted_nodes": to_string(get(depth, ["total_account_deleted_nodes"]) ?? 0) ?? "0",
        "total_account_deleted_bytes": to_string(get(depth, ["total_account_deleted_bytes"]) ?? 0) ?? "0",
        "total_storage_written_nodes": to_string(get(depth, ["total_storage_written_nodes"]) ?? 0) ?? "0",
        "total_storage_written_bytes": to_string(get(depth, ["total_storage_written_bytes"]) ?? 0) ?? "0",
        "total_storage_deleted_nodes": to_string(get(depth, ["total_storage_deleted_nodes"]) ?? 0) ?? "0",
        "total_storage_deleted_bytes": to_string(get(depth, ["total_storage_deleted_bytes"]) ?? 0) ?? "0",
        "account_written_nodes": get(depth, ["account_written_nodes"]) ?? {},
        "account_written_bytes": get(depth, ["account_written_bytes"]) ?? {},
        "account_deleted_nodes": get(depth, ["account_deleted_nodes"]) ?? {},
        "account_deleted_bytes": get(depth, ["account_deleted_bytes"]) ?? {},
        "storage_written_nodes": get(depth, ["storage_written_nodes"]) ?? {},
        "storage_written_bytes": get(depth, ["storage_written_bytes"]) ?? {},
        "storage_deleted_nodes": get(depth, ["storage_deleted_nodes"]) ?? {},
        "storage_deleted_bytes": get(depth, ["storage_deleted_bytes"]) ?? {}
      }
      .event_name = "EXECUTION_MPT_DEPTH"
      .event_time = format_timestamp!(now(), format: "%Y-%m-%dT%H:%M:%S%.fZ")

  # ============================================================================
  # Stage 7: Assemble Final DecoratedEvent
  # ============================================================================

  build_decorated_event:
    type: remap
    inputs:
      - build_block_metrics_event
      - build_state_size_delta_event
      - build_mpt_depth_event
    source: |
      network_id_str = to_string!(.network_id)
      event_name = string!(.event_name)

      event = {
        "event": {
          "name": event_name,
          "date_time": .event_time
        },
        "meta": {
          "client": {
            "name": .client_name,
            "version": .client_version,
            "implementation": .client_implementation,
            "ethereum": {
              "network": {
                "name": .network_name,
                "id": network_id_str
              }
            }
          }
        }
      }

      event = set!(event, [event_name], .event_data)
      . = { "events": [event] }

sinks:
  # HTTP sink to xatu-server
  xatu:
    type: http
    inputs: ["build_decorated_event"]
    uri: "${XATU_SERVER_URL}"
    method: post
    compression: "${XATU_COMPRESSION:-gzip}"
    encoding:
      codec: json
    request:
      headers:
        Content-Type: application/json
        Authorization: "Basic ${XATU_AUTH}"
    batch:
      max_events: ${XATU_BATCH_MAX_EVENTS:-5000}
      timeout_secs: ${XATU_BATCH_TIMEOUT_SECS:-5}

  # Debug sinks - uncomment for troubleshooting
  # debug_parsed:
  #   type: console
  #   inputs: ["parse_nested_json"]
  #   encoding:
  #     codec: json
  #
  # debug_normalized:
  #   type: console
  #   inputs: ["normalize_geth_block_metrics"]
  #   encoding:
  #     codec: json
  #
  # debug_final:
  #   type: console
  #   inputs: ["build_decorated_event"]
  #   encoding:
  #     codec: json
